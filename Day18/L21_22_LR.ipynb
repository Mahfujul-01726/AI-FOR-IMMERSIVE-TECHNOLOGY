{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression with one variable"
      ],
      "metadata": {
        "id": "oMb_AuoeY3rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will implement the linear regression and get to see it work on data. Linear Regression is the oldest and most widely used predictive model in the field of machine learning. The goal is to  minimize the sum of the squared errros to fit a straight line to a set of data points."
      ],
      "metadata": {
        "id": "pNEeraLwY-Uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering one example, we have a file that contains the dataset of our linear regression problem. The first column is the **population** of the city and the second column is the **profit** of having a store in that city. A negative value for profit indicates a loss.\n"
      ],
      "metadata": {
        "id": "y8jB-GPgaOvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Visualization**"
      ],
      "metadata": {
        "id": "i61DR0EmaWtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting, it is useful to understand the data by **visualizing** it.  We will use the **scatter plot** to visualize the data, since it has only two properties to plot (profit and population). Many other problems in real life are multi-dimensional and can't be plotted on 2-d plot."
      ],
      "metadata": {
        "id": "uRA6rhB-a2T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import loadtxt, zeros, ones, array, linspace, logspace, ones_like\n",
        "from pylab import scatter, show, title, xlabel, ylabel, plot, contour\n",
        "\n",
        "\n",
        "\n",
        "#Load the dataset\n",
        "data = loadtxt('ex1data1.txt')\n",
        "\n",
        "#Plot the data\n",
        "scatter(data[:, 0], data[:, 1], marker='o', c='b')\n",
        "title('Profits distribution')\n",
        "xlabel('Population of City in 10,000s')\n",
        "ylabel('Profit in $10,000s')\n",
        "show()"
      ],
      "metadata": {
        "id": "knKHvEuNam0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cost Function**"
      ],
      "metadata": {
        "id": "vO7Tv14Y4YWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you perform gradient descent to learn minimize the cost function J(θ), it is helpful to monitor the convergence by computing the cost. *(For Reference: Check **LinearRegression_GradientDescent.pdf** file)*"
      ],
      "metadata": {
        "id": "5UFFzqbidgGn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XajaoTYTWtF6"
      },
      "outputs": [],
      "source": [
        "#Evaluate the linear regression\n",
        "\n",
        "def compute_cost(X, y, theta):\n",
        "    '''\n",
        "    Comput cost for linear regression\n",
        "    '''\n",
        "    #Number of training samples\n",
        "    m = y.size\n",
        "\n",
        "    #### Start writing your code for Cost Computation here\n",
        "\n",
        "\n",
        "\n",
        "    return J"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Descent**"
      ],
      "metadata": {
        "id": "djVxP1NV4dWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you must fit the linear regression parameters to our dataset using **gradient descent**. The objective of linear regression is to minimize the **cost function**."
      ],
      "metadata": {
        "id": "U3nlvu9nqMZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Performs gradient descent to learn theta\n",
        "    by taking num_items gradient steps with learning\n",
        "    rate alpha\n",
        "    '''\n",
        "    m = y.size\n",
        "    J_history = zeros(shape=(num_iters, 1)) # To plot the convergence\n",
        "\n",
        "    for i in range(num_iters):\n",
        "\n",
        "\n",
        "        #### Start writing your code to compute Gradient Descent here\n",
        "\n",
        "\n",
        "\n",
        "    return theta, J_history"
      ],
      "metadata": {
        "id": "LnCgcdbPqOGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With each step of gradient  descent, your parameters θ, come close to the optimal values that will achieve the lowest cost J(θ)."
      ],
      "metadata": {
        "id": "yJENGbY_Wxt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Training**"
      ],
      "metadata": {
        "id": "AnFRsZiu4kux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our **initial inputs** we start with our **initial fitting parameters θ**, our data and add another dimmension to our data  to accommodate the $θ_0$ intercept term. As also our learning rate alpha to 0.01."
      ],
      "metadata": {
        "id": "dY8YZBYPtwFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[:, 0]\n",
        "y = data[:, 1]\n",
        "\n",
        "\n",
        "#number of training samples\n",
        "m = y.size\n",
        "\n",
        "#Add a column of ones to X (interception data)\n",
        "it = ones(shape=(m, 2))\n",
        "it[:, 1] = X\n",
        "\n",
        "#Initialize theta parameters\n",
        "theta = zeros(shape=(2, 1))\n",
        "\n",
        "#Some gradient descent settings\n",
        "iterations = 1500\n",
        "alpha = 0.01"
      ],
      "metadata": {
        "id": "kLEBPcuntQUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute and display initial cost\n",
        "print(compute_cost(it, y, theta))"
      ],
      "metadata": {
        "id": "-Yh_40ixvfhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute the theta values using gradient descent algorithm\n",
        "theta, J_history = gradient_descent(it, y, theta, alpha, iterations)\n",
        "\n",
        "print(theta)"
      ],
      "metadata": {
        "id": "rt2cgWVtxz5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Predictions**"
      ],
      "metadata": {
        "id": "5kQbzstX4PZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict values for population sizes of 35,000 and 70,000\n",
        "predict1 = array([1, 3.5]).dot(theta).flatten().item()\n",
        "print('For population = 35,000, we predict a profit of %f' % (predict1 * 10000))\n",
        "predict2 = array([1, 7.0]).dot(theta).flatten().item()\n",
        "print('For population = 70,000, we predict a profit of %f' % (predict2 * 10000))"
      ],
      "metadata": {
        "id": "99Ra6D_nyMFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming `data` and `theta` are already defined\n",
        "# Scatter plot of the data\n",
        "plt.scatter(data[:, 0], data[:, 1], marker='o', c='b', label='Training data')\n",
        "\n",
        "# Plot the regression line\n",
        "x_values = linspace(data[:, 0].min(), data[:, 0].max(), 100)  # Generate x values for the line\n",
        "y_values = array([ones_like(x_values), x_values]).T.dot(theta).flatten()  # Predict y for those x values\n",
        "\n",
        "plt.plot(x_values, y_values, color='r', label='Regression line')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Profit Distribution and Regression Line')\n",
        "plt.xlabel('Population of City in 10,000s')\n",
        "plt.ylabel('Profit in $10,000s')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j9QSzDEU1nYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Contour Plot**"
      ],
      "metadata": {
        "id": "Js9WP5FN4uR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good way to verify that gradient descent is working correctly is to look at the value of J(θ) and check that it is decreasing with each step. It should converge to a steady value by the end of the algorithm."
      ],
      "metadata": {
        "id": "Vb_xMykl4uQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another interesting plot is the **contour plots**, it will give you how J(θ) varies with changes in $θ_0$ and  $θ_1$.  The cost function J(θ) is bowl-shaped and has a global mininum as you can see in the figure below. Each step of gradient descent moves closer to this point."
      ],
      "metadata": {
        "id": "-jft7cth4_8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Grid over which we will calculate J\n",
        "theta0_vals = linspace(-10, 10, 100)\n",
        "theta1_vals = linspace(-1, 4, 100)\n",
        "\n",
        "\n",
        "#initialize J_vals to a matrix of 0's\n",
        "J_vals = zeros(shape=(theta0_vals.size, theta1_vals.size))\n",
        "\n",
        "#Fill out J_vals\n",
        "for t1, element in enumerate(theta0_vals):\n",
        "    for t2, element2 in enumerate(theta1_vals):\n",
        "        thetaT = zeros(shape=(2, 1))\n",
        "        thetaT[0][0] = element\n",
        "        thetaT[1][0] = element2\n",
        "        J_vals[t1, t2] = compute_cost(it, y, thetaT)\n",
        "\n",
        "#Contour plot\n",
        "J_vals = J_vals.T\n",
        "#Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\n",
        "contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))\n",
        "xlabel('theta_0')\n",
        "ylabel('theta_1')\n",
        "scatter(theta[0][0], theta[1][0])\n",
        "show()"
      ],
      "metadata": {
        "id": "1iSVMcGuvuVW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}